---
title: "EDS241: Assignment 2 Template"
author: "Elliott Finn"
date: '`r format(Sys.time(), "%m/%d/%Y")`'
output: 
  pdf_document:
    toc: false
    number_sections: yes
header-includes:
  \setlength{\parindent}{1em}
  \usepackage{float}
  \renewcommand{\thesubsection}{Question (\alph{subsection})}
--- 

**Reminders:** Make sure to read through the setup in markdown. Remember to write out interpretations and report your results in writing (and table/plot etc) forms.
``` {r setup, echo = FALSE, message = FALSE, warning = FALSE}
#Clean Environment
rm(list=ls())

# Setup your WD in a way that works for you- ideally a project that has all your scripts and data within it.
# getwd() check your WD

# Example of How I setup my coding
# I set an extension to retrieve data from a particular place. I keep my scripts on my computer, but data on servers to save space.
data_wd <- "/Users/elliottfinn/Library/CloudStorage/GoogleDrive-elliottfinn@ucsb.edu/Shared drives/EDS241/Assignments/Assignment 2"

# This makes it easier for me to load data
# data <- read_csv(paste0(data_wd,"/Assignment 2/EXTENSION_or_file"))

# set default chunk options
knitr::opts_chunk$set(fig.width = 4, fig.height = 3, 
                      echo = TRUE, message = FALSE, warning = FALSE)

# load packages
packages=c(
# Necessary for Assignment 2
  "Match","plm", "tidyverse", "MatchIt", "RItools", "Hmisc", "lmtest", "estimatr",
# You decide what works for you, these are the packages I use to display results ect, they may not be the ones you use.

"gridExtra", "stargazer", "kableExtra",
"purrr", "knitr", "broom",
   
  # Some Potentially useful packages from earlier examples
           "stargazer", "here", "tidyr", "dplyr","stringr", "janitor", 
           "cowplot", "ggplot2", "tinytex", "datasets", "tibble") # Used for Mock assignment

for (i in packages) {
  if (require(i,character.only=TRUE)==FALSE) {
    install.packages(i,repos='http://cran.us.r-project.org')
  }
  else {
    require(i,character.only=TRUE)
  }
}

# Disable scientific notation if you want
options(scipen=999)

```

# Part 1 Treatment Ignorability Assumption and Applying Matching Estimators (19 points):
The goal is to estimate the causal effect of maternal smoking during pregnancy on infant birth weight using the treatment ignorability assumptions. The data are taken from the National Natality Detail Files, and the extract “SMOKING_EDS241.csv”' is a random sample of all births in Pennsylvania during 1989-1991. Each observation is a mother-infant pair. The key variables are:

**The outcome and treatment variables are:**

\indent birthwgt=birth weight of infant in grams

\indent tobacco=indicator for maternal smoking

**The control variables are:**

\indent mage (mother's age), meduc (mother's education), mblack (=1 if mother identifies as Black), alcohol (=1 if consumed alcohol during pregnancy), first (=1 if first child), diabete (=1 if mother diabetic), anemia (=1 if mother anemic)

```{r , include=TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=60)}

# Load data for Part 1


```

## Mean Differences, Assumptions, and Covariates _(3 pts)_
a) What is the mean difference in birth weight of infants with smoking and non-smoking mothers? [1 pt] Under what assumption does this correspond to the average treatment effect of maternal smoking during pregnancy on infant birth weight? [0.5 pt] Calculate and create a table demonstrating the differences in the mean proportions/values of covariates observed in smokers and non-smokers (remember to report whether differences are statistically significant) and discuss whether this provides empirical evidence for or against this assumption. Remember that this is observational data. What other quantitative empirical evidence or test could help you assess the former assumption? [1.5 pt: 0.5 pt table, 1 pt discussion]

```{r , include=TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
## Calculate mean difference. Remember to calculate a measure of statistical significance

## For continuous variables you can use the t-test
#t.test()

## For binary variables you should use the proportions test
#prop.test()

## Covariate Calculations and Tables (feel free to use code from Assignment 1 key)

```

## ATE and Covariate Balance _(3 pts)_
b) Assume that maternal smoking is randomly assigned conditional on the observable covariates listed above. Estimate the effect of maternal smoking on birth weight using an OLS regression with NO linear controls for the covariates. [0.5 pts] Perform the same estimate including the control variables [0.5 pts]. Next, compute indices of covariate imbalance between the treated and non-treated regarding these covariates (see example file from class). Present your results in a table.[1 pts] What do you find and what does it say regarding whether the assumption you mentioned responding to a) is fulfilled? [1 pts]

```{r , include=TRUE, results = 'asis', tidy=TRUE, tidy.opts=list(width.cutoff=60)}

# ATE Regression univariate


# ATE with covariates


# Present Regression Results


# Covariate balance


# Balance Table 


```

## Propensity Score Estimation _(3 pts)_
c) Next, estimate propensity scores (i.e. probability of being treated) for the sample, using the provided covariates. Create a regression table reporting the results of the regression and discuss what the covariate coefficients indicate and interpret one coefficient [1.5 pts]. Create histograms of the propensity scores comparing the distributions of propensity scores for smokers ('treated') and non-smokers ('control'), discuss the overlap and what it means [1.5 pts].

```{r , include=TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=60)}

## Propensity Scores


## PS Histogram Unmatched 

```

## Matching Balance _(3 pts)_
(d) Next, match treated/control mothers using your estimated propensity scores and nearest neighbor matching. Compare the balancing of pretreatment characteristics (covariates) between treated and non-treated units in the original dataset (from c) with the matched dataset (think about comparing histograms/regressions) [2 pts]. Make sure to report and discuss the balance statistics [1 pts].

```{r , include=TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=60)}

## Nearest-neighbor Matching

## Covariate Imbalance post matching: 


## Histogram of PS after matching
```

## ATE with Nearest Neighbor _(3 pts)_
(e) Estimate the ATT using the matched dataset. Report and interpret your result (Note: no standard error or significance test is required here)

```{r , include=TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=60)}

## Nearest Neighbor 

## ATT


```

## ATE with WLS Matching _(3 pts)_
f) Last, use the original dataset and perform the weighted least squares estimation of the ATE using the propensity scores (including controls). Report and interpret your results, here include both size and precision of estimate in reporting and interpretation.

```{r , include=TRUE, results='asis', tidy=TRUE, tidy.opts=list(width.cutoff=60)}
## Weighted least Squares (WLS) estimator Preparation


## Weighted least Squares (WLS) Estimates


## Present Results

```

## Differences in Estimates _(1 pts)_ 
g) Explain why it was to be expected given your analysis above that there is a difference between your estimates in e) and f)? 



\newpage

# Part 2 Panel model and fixed effects (6 points)
\indent We will use the  progresa data (progresa.csv) from last time as well as a new dataset, progresa_pre.csv. In the original dataset, treatment households had been receiving the transfer for a year. Now, you get an additional dataset with information on the same households from before the program was implemented, establishing a baseline study (year 1997).
\indent *Note: You will need to install the packages plm and dplyr (included in template preamble). Again, you can find a description of the variables at the bottom of PDF and HERE.

## Estimating Effect with First Difference _(3 pts)_
Load the new baseline data (pre-program) and the follow-up data (post-program, from Assignment 1) into R. Create a time denoting variable (with the same name) in BOTH datasets with a value of 0 for the pre-program dataset and 1 for the other one. Create a panel dataset by appending the data (i.e. binding the dataset row-wise together creating a single dataset). We want to examine the same outcome variable as before, value of animal holdings (vani)-=. Estimate a standard difference-in-differences (DiD) regression and interpret the results.

```{r , include=TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
rm(list=ls()) # clean environment

## Load the datasets
# progresa_pre <- read.csv() insert your filepath etc
# progresa_post <- read.csv()

## Append post to pre dataset 
#progresa <- rbind(progresa_pre, progresa_post)

```
a) Estimate a first-difference (FD) regression manually, interpret the results briefly (size of coefficient and precision!)
\indent *Note: Calculate the difference between pre- and post-program for each individual and for each variable used (i.e the outcome and the independent variables).[3 pts] To do that, follow these steps and the code given in the R-template:

```{r, include=TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
### Code included to help get you started
## i. Sort the panel data in the order in which you want to take differences, i.e. by household and time.

## Create first differences of variables
# progresa <- progresa %>% 
#   arrange(hhid, year) %>% 
#   group_by(hhid)

## ii. Calculate the first difference using the lag function from the dplyr package.
#     mutate(vani_fd = vani - dplyr::lag(vani)) 

## iii. Estimate manual first-difference regression (Estimate the regression using the newly created variables.)
# fd_manual <- lm(vani_fd ~ ...)

```
## Fixed Effects Estimates _(2 pts)_
b) Now also run a fixed effects (FE or ‘within’) regression and compare the results. Interpret the estimated treatment effects briefly (size of coefficient and precision!)

```{r, include=TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
## Fixed Effects Regression

## Present Regression Results
```

## First Difference and Fixed Effects and Omitted Variable Problems _(1 pts)_
c) Explain briefly how the FD and FE estimator solves a specific omitted variable problem? Look at the example on beer tax and traffic fatalities from class to start thinking about ommitted variables. Give an example of a potential omitted variable for the example we are working with here that might confound our results? For that omitted variable, is a FE or FD estimator better? One example is enough.

